<!DOCTYPE html>
<html lang="en">

<head>
	<title>The virtual soundscape - Phenomena</title>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
	<link type="text/css" rel="stylesheet" href="main.css">
	<link rel="shortcut icon" href="https://p3-blog.corruptedpixl.com/assets/favicons/favicon.svg">
</head>

<body>

	<div id='audioContainer'></div>
	<div id='videoContainer'></div>

	<div id="overlay">
		<h1>Welcome to Phenomena: behind the music</h1>
		<p>This WebXR experience will show you how Phenomena came to be and allow you to play with the songs.</p>
		<button id="startButton">Start the experience</button>
		<p id="loading">Loading...</p>
		<p>You can view the experience in your browser by pressing this button or in VR by pressing the "enter VR" button at the bottom once the experience is started.</p>
		<p>Special thanks to Zanski for allowing me to use the stems, artwork and videos for this project!</p>
	</div>

	<script type="module">

		import { Text } from 'https://cdn.skypack.dev/troika-three-text';

		import * as THREE from 'https://cdn.skypack.dev/three';
		import { GUI } from 'https://cdn.skypack.dev/three/examples/jsm/libs/lil-gui.module.min.js';
		import { FirstPersonControls } from 'https://cdn.skypack.dev/three/examples/jsm/controls/FirstPersonControls.js';
		import { VRButton } from 'https://cdn.skypack.dev/three/examples/jsm/webxr/VRButton.js';
		import { XRControllerModelFactory } from 'https://cdn.skypack.dev/three/examples/jsm/webxr/XRControllerModelFactory.js';
		import { GLTFLoader } from 'https://cdn.skypack.dev/three/examples/jsm/loaders/GLTFLoader.js';

		import StarShader from './shaders/StarShader.js';
		import FloorShader from './shaders/FloorShader.js';

		let camera, controls, scene, raycaster, renderer, light, listener, textureEquirec, waterTexture, noteObject, skyMaterial;
		let controller, controllerGrip;
		let playListJson, albumArt, albumTitle, artist, lyrics;
		let currentStems = [];
		let currentLyrics = [];
		let globalTime = { value: 0 };
		const tempMatrix = new THREE.Matrix4();
		const text = new Text();

		let mats = [];
		let meshes = [];
		let sounds = [];
		let videos = [];
		let analysers = [];
		let HDRIS = [];

		let controllers = [];
		let controllerGrips = [];

		// groups
		let stemsGroup = new THREE.Group();
		let vidGroup = new THREE.Group();
		let songSelectGroup = new THREE.Group();

		const clock = new THREE.Clock();
		const textureLoader = new THREE.TextureLoader();

		const startButton = document.getElementById('startButton');
		startButton.addEventListener('click', init);

		// const vidConfig = ['video', 'video2', 'video3']
		const vidConfig = []
		const HDRIConfig = ['dark_sat', 'sat2', 'dark_light', 'light_darker', 'light_darkerer', 'light_darkerer_oculus', 'dark_light_oculus'] // TODO remove all but two

		const refDist = 50; // distance from where to start applying volume reduction 

		const getPlaylist = async (url) => { return await fetch(url).then(response => response.json()); }

		const initPlaylist = async () => {
			playListJson = await getPlaylist('media/json/playlist.json');
			albumArt = playListJson.albumArt;
			albumTitle = playListJson.album;
			artist = playListJson.artist;

			albumArt = preloadImgs([albumArt]); /// allows to load multiple sources if needed
			preloadVideos(playListJson.videos); /// loads all videos and appends them to the dom to access later
			preloadAudio()
		}

		const preloadImgs = async (imgs) => {
			/// Whether or not this image is actually used or not does not matter, since it'll be cached anyway
			imgs.forEach(async img => {
				const image = new Image();
				image.src = img;
				return image;
			})
		}

		const preloadVideos = (videos) => {
			const $videoContainer = document.querySelector('#videoContainer');

			videos.forEach(video => { vidConfig.push(video.id) })

			$videoContainer.innerHTML = videos.map(video => `
			<video id="${video.id}" preload="auto" style="display: none" crossorigin="anonymous" muted controls loop>
				<source src="${video.url}" type="video/mp4" crossorigin="*">
			</video>`
			).join(``)
		}

		const preloadAudio = () => {
			const defaultTrack = 9;
			loadTrack(playListJson.tracks[defaultTrack]);
		}

		const loadTrack = (track) => {
			return new Promise((resolve, reject) => {
				currentStems = [];
				const $audioContainer = document.querySelector('#audioContainer');
				console.log('%c%s', 'color: #9FE3FF', `Now loading: ${track.title}`);

				track.stems.forEach(stem => {
					const audio = new Audio();
					audio.src = `${track.stemsPath}${stem.name}.mp3`;
					audio.loop = true;
					audio.name = stem.name;
					$audioContainer.appendChild(audio);

					/// This event fires when the browser thinks it can play the file without having to buffer, is not always reliable but it's the best we can do.
					audio.oncanplaythrough = (e) => {
						const $loading = document.querySelector('#loading');
						currentStems[audio.name] = { ...stem, "audio": audio }; /// Add the audio object to the existing currentStems object
						console.log('%c%s', 'color: #4AF2FF', `Loaded ${audio.name}`);
						if (Object.keys(currentStems).length === track.stems.length) {
							if ($loading) $loading.innerHTML = 'Loaded all stems!'
							console.log('%c%s', 'color: #1F8BFF', `All stems loaded.`);
							resolve();
						}
					};
				})
			})
		}

		const unloadAudio = () => {

			return new Promise((resolve, reject) => {
				/// Remove all previous audio files
				const $audioContainer = document.querySelector('#audioContainer');
				$audioContainer.innerHTML = '';

				const stemsGroup = scene.getObjectByProperty('name', 'stemsGroup');
				let stemsToBeRemoved = [...stemsGroup.children]; /// Make a clone of the children to loop over otherwise the array will shorten during the loop and this skip every 2nd item

				for (const stem in stemsToBeRemoved) {
					const stemObject = stemsToBeRemoved[stem];
					/// We dispose of the geometry and materials first to free up memory in Three.js
					stemObject.geometry.dispose();
					stemObject.material.dispose();

					/// Then we remove these entries fron the objects we stored them in
					delete mats[stemsToBeRemoved[stem].name]
					delete meshes[stemsToBeRemoved[stem].name]
					delete sounds[stemsToBeRemoved[stem].name]
					delete analysers[stemsToBeRemoved[stem].name]

					/// And finally remove the object itself from the scene
					stemObject.removeFromParent();
				}
				stemsToBeRemoved = [];
				scene.remove(stemsGroup);
				renderer.renderLists.dispose(); /// Force Three.js to clear up the memory

				resolve();
			})
		}

		initPlaylist(); /// Fetch the playlist data and preload images, sounds and videos

		const initScene = () => {
			return new Promise((resolve, reject) => {
				camera = new THREE.PerspectiveCamera(70, window.innerWidth / window.innerHeight, .1, 10000);
				camera.position.set(0, 20, 0);

				listener = new THREE.AudioListener();
				camera.add(listener);

				scene = new THREE.Scene();
				// scene.fog = new THREE.FogExp2(0x000000, 0.005);

				/// Lights for the notes
				const light = new THREE.DirectionalLight(0xffffbb);
				light.position.set(100, 30, -100).normalize();
				light.castShadow = true;
				scene.add(light);

				const light1 = new THREE.PointLight(0xffffff, .03, 0);
				light1.position.set(0, 200, 0);
				scene.add(light1);

				const light2 = new THREE.PointLight(0xffffff, .01, 0);
				light2.position.set(100, 10, 50);
				scene.add(light2);

				// /// Ground grid
				// const helper = new THREE.GridHelper(1000, 10, 0x444444, 0x444444);
				// helper.position.y = 0.1;
				// scene.add(helper);

				/// Raycaster for controllers
				raycaster = new THREE.Raycaster();

				/// Render setup
				renderer = new THREE.WebGLRenderer({ antialias: true });
				renderer.setPixelRatio(window.devicePixelRatio);
				renderer.setSize(window.innerWidth, window.innerHeight);
				renderer.outputEncoding = THREE.sRGBEncoding;
				renderer.shadowMap.enabled = true;
				/// enable webxr
				renderer.xr.enabled = true;
				document.body.appendChild(renderer.domElement);

				resolve();
			})
		}

		const initStems = (currentStems) => {
			/// Loading GLTF note model
			const loader = new GLTFLoader().setPath('models/');
			loader.load('note_gold.gltf', (gltf) => {
				const noteModel = gltf.scene;
				noteObject = noteModel.children[0].geometry; /// Get the raw geometry
				loadStems(currentStems).then(() => { initGui(); }); /// Now that the model is fully loaded, we can continue to load the stems and then the GUI, which depends on the stems
			});
		}

		const loadStems = (currentStems) => {

			return new Promise((resolve, reject) => {
				const populateArrays = new Promise((resolve, reject) => {
					for (const stem in currentStems) {
						// Create a new material with the color of the object
						mats[currentStems[stem].name] = new THREE.MeshPhongMaterial({ color: currentStems[stem].color, flatShading: true, shininess: .1, specular: currentStems[stem].color });
						// Create a new mesh with that material and the note model
						meshes[currentStems[stem].name] = new THREE.Mesh(noteObject, mats[currentStems[stem].name])
						meshes[currentStems[stem].name].name = currentStems[stem].name;
						meshes[currentStems[stem].name].rotateY(Math.PI);
						stemsGroup.add(meshes[currentStems[stem].name]); /// Add it to a group to make life easier later on when we load new stems

						// Create audio sources, load from html and add to stems
						sounds[currentStems[stem].name] = new THREE.PositionalAudio(listener);
						const currentSound = currentStems[stem].audio;
						sounds[currentStems[stem].name].setMediaElementSource(currentSound).setRefDistance(refDist);
						meshes[currentStems[stem].name].add(sounds[currentStems[stem].name]);

						analysers[currentStems[stem].name] = new THREE.AudioAnalyser(sounds[currentStems[stem].name], 32); // Start analysers
					}

					resolve()
				})

				populateArrays.then(res => {
					let i = 0;
					const spacer = 50;
					// set positions of objects
					for (const mesh in meshes) {
						meshes[mesh].position.set((spacer * 2 - (spacer * i)), 30, 100)
						meshes[mesh].scale.set(.7, .7, .7)
						i++
					}
				})

				stemsGroup.name = 'stemsGroup';
				scene.add(stemsGroup); // Add stems group to the scene

				for (const stem in currentStems) { currentStems[stem].audio.play() } /// Finally start playing them all at the same time (or as close as possible)
				resolve();
			})
		}

		const initVideoGroup = (vidConfig) => {
			const vidTextures = [];

			/// Create video materials from the preloaded vids in the html
			vidConfig.forEach(video => {
				const currentVid = document.querySelector(`#${video}`);
				videos.push(currentVid);
				vidTextures.push(new THREE.MeshBasicMaterial({ map: new THREE.VideoTexture(currentVid), }));
			});

			/// Create a triangle for the sides of the object
			const triangleSize = .5;
			const triangle = new THREE.Shape(); // create a triangle
			triangle.moveTo(10 * triangleSize, 0); // right point
			triangle.lineTo(-9.4 * triangleSize, 0); // left point
			triangle.lineTo(0.16, 16.8 * triangleSize); // top point
			triangle.lineTo(10 * triangleSize, 0); // back to start to close the shape

			const vidShapeSideOne = new THREE.Mesh(
				new THREE.ExtrudeGeometry(triangle, { steps: 1, depth: .1, }),
				new THREE.MeshBasicMaterial({ color: 0xffbb00 })
			)
			const vidShapeSideTwo = new THREE.Mesh(
				new THREE.ExtrudeGeometry(triangle, { steps: 1, depth: .1, }),
				new THREE.MeshBasicMaterial({ color: 0xffbb00 })
			)
			vidShapeSideOne.position.set(10, 10.08, -14.85); /// Align perfectly with videos
			vidShapeSideTwo.position.set(-10, 10.08, -14.85);
			vidShapeSideOne.rotateY(Math.PI / 2); // 90° radians
			vidShapeSideTwo.rotateY(Math.PI / 2);

			/// Video mesh group
			let vidSideAlpha = new THREE.Mesh(
				new THREE.PlaneGeometry(20, 10),
				vidTextures[0]);
			vidSideAlpha.name = 'vidSideAlpha';
			vidSideAlpha.position.set(0, 14.33, -17.5);
			vidSideAlpha.rotateY(Math.PI); // 180° radians
			vidSideAlpha.rotateZ(Math.PI); // 180° radians
			vidSideAlpha.rotateX(0.523599); // 30° in radians (formula deg to rad: angle * π / 180)
			vidSideAlpha.material.side = THREE.DoubleSide; // render both sides

			let vidSideBeta = new THREE.Mesh(
				new THREE.PlaneGeometry(20, 10),
				vidTextures[1]);
			vidSideBeta.name = 'vidSideBeta';
			vidSideBeta.position.set(0, 10, -15);
			vidSideBeta.rotateX(1.5708); // 90° in radians

			let vidSideGamma = new THREE.Mesh(
				new THREE.PlaneGeometry(20, 10),
				vidTextures[2]);
			vidSideGamma.name = 'vidSideGamma';
			vidSideGamma.position.set(0, 14.33, -12.5);
			vidSideGamma.rotateX(-0.523599); // 30° in radians

			/// Add them all together
			vidGroup.add(vidSideAlpha);
			vidGroup.add(vidSideBeta);
			vidGroup.add(vidSideGamma);
			vidGroup.add(vidShapeSideOne);
			vidGroup.add(vidShapeSideTwo);
			vidGroup.userData.isDraggable = true; /// Enable dragging (custom property)
			vidGroup.name = 'vidGroup';
			vidGroup.scale.set(.1, .1, .1);
			vidGroup.position.set(1.2774114085134398, 0.6355997895828723, -0.4851377603984951) /// I aligned the object in VR and then logged the coordinates, this is why they might seem random
			vidGroup.rotation.set(-0.46588650970294515, -1.1860233074386377, -0.44803679405802344) /// Same but with Euler angles

			scene.add(vidGroup);

			videos.forEach(video => {
				video.load();
				video.play();
			});
		}

		const initGui = () => {
			const SoundControls = function () {
				this.master = listener.getMasterVolume();
				for (const stem in currentStems) { this[currentStems[stem].name] = sounds[currentStems[stem].name].getVolume(); }
			};

			const gui = new GUI();
			const soundControls = new SoundControls();
			const volumeFolder = gui.addFolder('sound volume');

			volumeFolder.add(soundControls, 'master').min(0.0).max(1.0).step(0.01).onChange(function () {
				listener.setMasterVolume(soundControls.master);

				// if (soundControls.master < .3) { scene.background = HDRIS.light_darkerer; }
				// if (soundControls.master > .3 && soundControls.master < .6) { scene.background = HDRIS.dark_light; }
				// if (soundControls.master > .6 && soundControls.master < .8) { scene.background = HDRIS.sat2; }
				// if (soundControls.master > .8) { scene.background = HDRIS.dark_sat; }
			});

			// add controls for each sound loaded
			for (const stem in currentStems) {
				volumeFolder.add(soundControls, currentStems[stem].name, 0, 1, 0.01).onChange(value => {
					sounds[currentStems[stem].name].setVolume(value);
				});
			}
			volumeFolder.open(); /// Open the GUI controls for desktop users.
		}

		const initControllers = () => {
			/// Callback functions for when events fire
			function onSelectStart(e) {
				this.userData.isSelecting = true;
				const controller = e.target;

				tempMatrix.identity().extractRotation(controller.matrixWorld);
				raycaster.ray.origin.setFromMatrixPosition(controller.matrixWorld);
				raycaster.ray.direction.set(0, 0, - 1).applyMatrix4(tempMatrix);

				///! Could be optimized slightly by adding all draggable objects to a group and only checking intersections on that group instead of the whole scene
				const intersects = raycaster.intersectObjects(scene.children, false);
				const intersectsVidGroup = raycaster.intersectObjects(vidGroup.children, false);
				const intersectsSongSelect = raycaster.intersectObjects(songSelectGroup.children, false);

				// /// Standalone objects
				// if (intersects.length > 0) {
				// 	intersects.forEach(obj => {
				// 		if (!obj.object.userData?.isDraggable) { return } /// Early return if object is not draggable
				// 		else if (obj.object.parent.userData?.isDraggable) { /// To attach grouped objects like the video group. This needs to be here or you'd only move the faces of the video group
				// 			const object = obj.object;
				// 			controller.attach(object);
				// 			controller.userData.selected = object;
				// 		}
				// 		else { /// For all other objects
				// 			const object = obj.object;
				// 			controller.attach(object);
				// 			controller.userData.selected = object;
				// 		}
				// 	})
				// }

				/// Double click (un)mute video checks
				if (intersectsVidGroup.length > 0) {
					if (this.userData?.videoGroup?.lastVideoObj === intersectsVidGroup[0].object && performance.now() - this.userData?.videoGroup?.lastClick <= 400) { /// Two clicks within the last [400]ms?
						switch (intersectsVidGroup[0].object.name) {
							case 'vidSideAlpha':
								videos[0].muted = !videos[0].muted;
								break;
							case 'vidSideBeta':
								videos[1].muted = !videos[1].muted;
								break;
							case 'vidSideGamma':
								videos[2].muted = !videos[2].muted;
								break;
							default:
								break;
						}
					}

					intersectsVidGroup.forEach(obj => {
						if (!obj.object.parent.userData?.isDraggable) { return } /// Early return if object is not draggable
						else if (obj.object.parent.userData?.isDraggable) { /// To attach grouped objects like the video group. This needs to be here or you'd only move the faces of the video
							const object = obj.object.parent;
							controller.attach(object);
							controller.userData.selected = object;
						}
						else {
							const object = obj.object;
							controller.attach(object);
							controller.userData.selected = object;
						}
					})

					this.userData.videoGroup = {
						lastVideoObj: intersectsVidGroup[0].object,
						lastClick: performance.now()
					}
				}


				/// Song selector
				if (intersectsSongSelect.length > 0) {
					if (!playListJson.tracks.some(track => track.title === intersectsSongSelect[0].object.name)) { return } /// Are we selecting a song that is part of the current album?
					else {
						const selectedTrack = playListJson.tracks.findIndex(track => track.title === intersectsSongSelect[0].object.name);
						/// Load new stems
						unloadAudio()
							.then(() => loadTrack(playListJson.tracks[selectedTrack]))
							.then(() => loadStems(currentStems))
							.then(() => loadLyrics(playListJson.tracks[selectedTrack]))
					}
				}

			}

			function onSelectEnd(e) {
				/// 'this' is the controller group
				this.userData.isSelecting = false;
				const controller = e.target;

				if (controller.userData.selected !== undefined) {
					const object = controller.userData.selected;
					scene.attach(object); /// Attach the obj back to the scene (free from controller)
					controller.userData.selected = undefined;
				}
			}

			/// Init & render controller models
			/// Populate the groups
			controllers = [renderer.xr.getController(0), renderer.xr.getController(1)]
			controllerGrips = [renderer.xr.getControllerGrip(0), renderer.xr.getControllerGrip(1)]
			const controllerModelFactory = new XRControllerModelFactory();

			/// Adding the event listeners
			controllers.forEach(controller => {
				controller.addEventListener('selectstart', onSelectStart);
				controller.addEventListener('selectend', onSelectEnd);
				controller.addEventListener('connected', function (event) {
					this.add(buildController(event.data));
				});
				controller.addEventListener('disconnected', function () {
					this.remove(this.children[0]);
				});
				scene.add(controller);
			})

			/// Adding models so they're visible
			controllerGrips.forEach(controllerGrip => {
				controllerGrip.add(controllerModelFactory.createControllerModel(controllerGrip));
				scene.add(controllerGrip);
			});
		}

		const loadLyrics = (track) => {
			/// Reset the timer and lyrics obj
			lyrics = track.lyrics;
			lyrics.startTime = performance.now();
			lyrics.lineIndex = 0;
			lyrics.ended = false;
			lyrics.timeOffset = track.lyrics.timeOffset || 0;
		}

		const initLyrics = (track) => {
			loadLyrics(track) /// Get the lyrics, reset the object and restart the timer
			// Set properties to configure:
			text.text = 'Loading lyrics'
			text.fontSize = 3
			text.font = 'https://cdn.cpixl.com/fonts/next/OTF/Next%20Book/NextBook-Medium.otf'
			text.position.set(-40, 10, -15)
			text.rotateY(Math.PI / 2.5)
			text.color = 0xffbb00
			text.anchorX = 'center';

			scene.add(text)

			text.sync() // Force update the rendering
		}

		const initSongSelect = (playlist) => {
			/// Add title of each track
			const tracks = playlist.tracks;
			for (const track in tracks) {
				const trackText = new Text();
				trackText.text = tracks[track].title;
				trackText.name = tracks[track].title
				trackText.fontSize = 2
				trackText.font = 'https://cdn.cpixl.com/fonts/next/OTF/Next%20Book/NextBook-Medium.otf'
				trackText.position.set(0, - track * 2.5, 0)
				trackText.color = 0xffffff
				trackText.sync() // Force update the rendering
				songSelectGroup.add(trackText)
			}

			/// Add name of the album
			const albumText = new Text();
			albumText.text = playListJson.album;
			albumText.name = playListJson.album;
			albumText.fontSize = 3;
			albumText.font = 'https://cdn.cpixl.com/fonts/next/OTF/Next%20Book/NextBook-Bold.otf';
			albumText.position.set(-22, -21, 0);
			songSelectGroup.add(albumText);

			/// Add name of the artist
			const artistText = new Text();
			artistText.text = playListJson.artist;
			artistText.name = playListJson.artist;
			artistText.fontSize = 2;
			artistText.font = 'https://cdn.cpixl.com/fonts/next/OTF/Next%20Book/NextBook-Regular.otf';
			artistText.position.set(-22, -24, 0);
			songSelectGroup.add(artistText);

			/// Add the album art
			let albumArtTexture = textureLoader.load(playListJson.albumArt);
			albumArtTexture.encoding = THREE.sRGBEncoding; ///! THIS IS REALLY IMPORTANT. Since we set the renderer to sRGB, we need to set the texture to sRGB too otherwise the colors WILL BE WRONG!

			const albumArtPlane = new THREE.Mesh(
				new THREE.PlaneGeometry(20, 20),
				new THREE.MeshBasicMaterial({ map: albumArtTexture, side: THREE.DoubleSide })
			);
			albumArtPlane.position.set(-12, -10, 0);
			songSelectGroup.add(albumArtPlane)

			songSelectGroup.name = 'songSelectGroup';
			songSelectGroup.position.set(0, 40, -50);
			scene.add(songSelectGroup);
		}

		const initFloor = () => {
			// /// Create floor shader material
			// const material = new THREE.ShaderMaterial({
			// 	uniforms: {
			// 		time: globalTime,
			// 		resolution: { value: new THREE.Vector2(window.innerWidth, window.innerHeight) }
			// 	},
			// 	vertexShader: FloorShader.vertex,
			// 	fragmentShader: FloorShader.fragment
			// });

			/// Uncomment for Oculus build (replaces shader with static texture for performance)
			const floorTex = textureLoader.load(`textures/floor.png`);
			floorTex.magFilter = THREE.NearestFilter;
			floorTex.wrapT = THREE.RepeatWrapping;
			floorTex.wrapS = THREE.RepeatWrapping;
			floorTex.repeat.set(1, 1);
			const material = new THREE.MeshBasicMaterial({ map: floorTex, side: THREE.DoubleSide });

			const FloorShaderPlane = new THREE.Mesh(new THREE.PlaneGeometry(10000, 10000), material);
			scene.add(FloorShaderPlane);
			FloorShaderPlane.rotateX(- Math.PI / 2)
			FloorShaderPlane.position.set(0, -150, 0)

			// /// Create starry sky shader material
			// const skyDomeRadius = 5000.01; /// Since this is used in calculations inside the shader, we add a .01 to make sure its a float and not an int.
			// skyMaterial = new THREE.ShaderMaterial({
			// 	uniforms: {
			// 		time: { value: 0 },
			// 		skyRadius: { value: skyDomeRadius },
			// 		env_c1: { value: new THREE.Color('#000000') }, /// set to black for blending later
			// 		env_c2: { value: new THREE.Color('#000000') }, /// set to black for blending later
			// 		noiseOffset: { value: new THREE.Vector3(100.01, 100.01, 100.01) },
			// 		starSize: { value: 0.004 },
			// 		starDensity: { value: 0.09 },
			// 		clusterStrength: { value: 0.1 },
			// 		clusterSize: { value: 0.2 },
			// 	},
			// 	vertexShader: StarShader.vertex,
			// 	fragmentShader: StarShader.fragment,
			// 	side: THREE.DoubleSide, /// We're viewing the inside so we need to render both sides
			// 	transparent: true, /// Doesnt actually do anything but let Three.js know we want to do something with transparency. This tells the render to render possible transparency.
			// 	blending: THREE.AdditiveBlending, /// Here we actually create transparent pixels, wont work without 'transparent' being set to true
			// });

			// const sphereGeometry = new THREE.SphereGeometry(skyDomeRadius, 20, 20);
			// const skyDome = new THREE.Mesh(sphereGeometry, skyMaterial);
			// scene.add(skyDome);

			/// Load HDRI's
			HDRIConfig.forEach(HDRI => {
				const currentHDRI = textureLoader.load(`textures/hdri-${HDRI}.png`);
				currentHDRI.mapping = THREE.EquirectangularReflectionMapping;
				currentHDRI.encoding = THREE.sRGBEncoding;
				HDRIS[HDRI] = currentHDRI;
			})

			/// Load water texture for glass floor
			waterTexture = textureLoader.load(`textures/waternormals.jpg`);
			waterTexture.magFilter = THREE.NearestFilter;
			waterTexture.wrapT = THREE.RepeatWrapping;
			waterTexture.wrapS = THREE.RepeatWrapping;
			waterTexture.repeat.set(800, 2000.5);

			scene.background = HDRIS.dark_light_oculus; /// Apply the HDRI as the background of the scene. This renders it correctly since we mapped it to the equirectangular mapping.

			// /// Glass floor physical material
			// const glassPlane = new THREE.Mesh(
			// 	new THREE.PlaneGeometry(2000, 2000),
			// 	new THREE.MeshPhysicalMaterial({
			// 		color: 0xffffff,
			// 		metalness: 0,
			// 		roughness: 1,
			// 		ior: 1.1,
			// 		alphaMap: waterTexture,
			// 		envMap: HDRIS.dark_light,
			// 		envMapIntensity: 1,
			// 		transmission: 1,
			// 		specularIntensity: 1,
			// 		specularColor: 0xffffff,
			// 		opacity: 1,
			// 		thickness: 2,
			// 		side: THREE.DoubleSide,
			// 		transparent: true,
			// 		clearcoat: 1,
			// 		clearcoatMap: waterTexture,

			// 	}))

			// glassPlane.rotateX(- Math.PI / 2)
			// scene.add(glassPlane);
		}

		const buildController = (data) => {
			/// Native three.js code that creates the controller or dot on the screen in case of Google Cardboard VR
			let geometry, material;
			switch (data.targetRayMode) {
				case 'tracked-pointer':
					geometry = new THREE.BufferGeometry();
					geometry.setAttribute('position', new THREE.Float32BufferAttribute([0, 0, 0, 0, 0, - 1], 3));
					geometry.setAttribute('color', new THREE.Float32BufferAttribute([0.5, 0.5, 0.5, 0, 0, 0], 3));
					material = new THREE.LineBasicMaterial({ vertexColors: true, blending: THREE.AdditiveBlending });
					return new THREE.Line(geometry, material);
				case 'gaze':
					geometry = new THREE.RingGeometry(0.02, 0.04, 32).translate(0, 0, - 1);
					material = new THREE.MeshBasicMaterial({ opacity: 0.5, transparent: true });
					return new THREE.Mesh(geometry, material);
			}
		}

		const onWindowResize = () => {
			camera.aspect = window.innerWidth / window.innerHeight;
			camera.updateProjectionMatrix();
			renderer.setSize(window.innerWidth, window.innerHeight);
			controls.handleResize();
		}

		const animate = () => {
			/// For desktop rendering 'requestAnimationFrame(animate)' would be used, but this needs to change to the below for VR and related stuff. Keeps working on desktop too though. 
			renderer.setAnimationLoop(render);
		}

		const render = () => {
			const delta = clock.getDelta();
			controls.update(delta);

			globalTime.value = clock.elapsedTime; /// Floor shader
			// skyMaterial.uniforms.time.value = clock.getElapsedTime(); /// Sky shader

			/// Analysers
			for (const obj in stemsGroup.children) {
				if (!stemsGroup.children[obj]?.isHighlighted) {
					/// Update the red emissive channel, this seems to be the most visible one of the tree
					mats[stemsGroup.children[obj].name].emissive.r = analysers[stemsGroup.children[obj].name].getAverageFrequency() / 256 * 2;

					/// Change HDRI based on volume (flash on loud drum hits)
					if (stemsGroup.children[obj].name === 'drums' || stemsGroup.children[obj].name === 'FSS-drums') {
						const data = analysers[stemsGroup.children[obj].name].getFrequencyData() /// Get the raw data array

						/// Use the first 3 data points and normalize the volume. The first three are the lowese frequencies so this will only fire when kicks or low snares are hit. We dont want to fire on each hi-hat, that would be too much. This is a nice balance.
						if ((data[0] / 255 + data[1] / 255 + data[2] / 255) / 3 >= .85) { /// [.85] is the normalized threshold
							scene.background = HDRIS.light_darkerer_oculus
						} else if (scene.background !== HDRIS.dark_light_oculus) {
							scene.background = HDRIS.dark_light_oculus
						}
					}
				}
			}

			/// Lyrics
			lyrics.currentTime = performance.now();
			lyrics.timeDelta = parseInt(lyrics.currentTime - lyrics.startTime);

			/// Check if between the last frame and now a line needs to be shown based on the timing in the lyrics json
			if ((lyrics.timeDelta + lyrics.timeOffset) >= lyrics.lines[lyrics.lineIndex]?.startTimeMs && !lyrics.ended) {
				text.text = lyrics.lines[lyrics.lineIndex].words;
				text.sync();
				lyrics.lineIndex++;
			}
			if (typeof lyrics.lines[lyrics.lineIndex] == 'undefined') { lyrics.ended = true } /// Stop checking the timing each frame if the lyrics have ended. Every little bit of performance counts

			/// Controller code
			/// Find intersections
			controllers.forEach(controller => {
				/// 'controller' is the controller object aka 'this' in the event listeners
				tempMatrix.identity().extractRotation(controller.matrixWorld);
				raycaster.ray.origin.setFromMatrixPosition(controller.matrixWorld);
				raycaster.ray.direction.set(0, 0, - 1).applyMatrix4(tempMatrix);
				const intersects = raycaster.intersectObjects(stemsGroup.children, false);

				if (intersects.length > 0) {
					/// Is the object we're pointing at the same as the previous frame?
					if (controller.userData.INTERSECTED != intersects[0].object) {
						/// If not, remove the highlight from the previous object (if it exists)
						if (controller.userData.INTERSECTED != undefined) { controller.userData.INTERSECTED.isHighlighted = false; }
						/// Set the color of the old object back to what it was
						if (controller.userData.INTERSECTED) controller.userData.INTERSECTED.material.emissive.setHex(controller.userData.INTERSECTED.currentHex);
						controller.userData.INTERSECTED = intersects[0].object; /// Set the current object to be the new INTERSECTED object
						controller.userData.INTERSECTED.currentHex = controller.userData.INTERSECTED.material.emissive.getHex();/// Get the current color to revert to later when deselecting
						controller.userData.INTERSECTED.material.emissive.setHex(0xffbb00); /// Modify the color of the object (apply highlight)
						controller.userData.INTERSECTED.isHighlighted = true; /// Finally set object state to be highlighted
					}

					/// Is this the first time we're clicking on the object? (and are we not dragging an object)
					if (controller.userData.isSelecting && !controller.userData.isHolding && !controller.userData.selected) {
						/// Set for next frame
						controller.userData.isHolding = true;
						controller.userData.intent = { holdLog: [], action: 0 };

						const volume = sounds[controller.userData.INTERSECTED.name].getVolume();
						switch (volume) {
							case 0:
								sounds[controller.userData.INTERSECTED.name].setVolume(1);
								controller.userData.intent.action = 1; /// Update action if unmuting
								break;
							case 1:
								sounds[controller.userData.INTERSECTED.name].setVolume(0);
								break;
							default:
								sounds[controller.userData.INTERSECTED.name].setVolume(0);
								break;
						}
					} else if (!controller.userData.isSelecting && controller.userData.isHolding) {
						controller.userData.isHolding = false;
					}

					/// Checks for user intent
					if (controller.userData.isSelecting && controller.userData.isHolding && !controller.userData.selected) {
						if (controller.userData.intent?.holdLog.at(-1) != intersects[0].object) {
							if (controller.userData.intent.holdLog == undefined) {
								controller.userData.intent.holdLog = [];
							}
							controller.userData.intent.holdLog.push(intersects[0].object)
						} else {
							sounds[controller.userData.INTERSECTED.name].setVolume(controller.userData.intent.action);
						}
					}

				} else {
					/// Clean up and reset currently highlighted objects to their original color/state
					if (controller.userData.INTERSECTED != undefined) { controller.userData.INTERSECTED.isHighlighted = false; }
					if (controller.userData.INTERSECTED) controller.userData.INTERSECTED.material.emissive.setHex(controller.userData.INTERSECTED.currentHex);
					controller.userData.INTERSECTED = undefined;
				}
			})
			/// Render the scene
			renderer.render(scene, camera);
		}

		function init() {
			/// Remove the overlay
			const overlay = document.getElementById('overlay');
			overlay.remove();

			/// Create and load the scene, stems, lyrics etc...
			initScene()
				.then(() => initStems(currentStems))
				.then(() => initVideoGroup(vidConfig))
				.then(() => initControllers())
				.then(() => initLyrics(playListJson.tracks[9]))
				.then(() => initFloor())
				.then(() => initSongSelect(playListJson))

			/// Add desktop controls
			controls = new FirstPersonControls(camera, renderer.domElement);
			controls.movementSpeed = 70;
			controls.lookSpeed = 0.30;
			controls.noFly = false;
			controls.lookVertical = true;

			window.addEventListener('resize', onWindowResize);
			document.body.appendChild(VRButton.createButton(renderer));

			animate();
		}
	</script>

</body>

</html>